\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref} % Add this package for hyperlinks

\title{Advanced NLP Exercise 1}
\author{Dean Tahory}
\date{}

\begin{document}

\maketitle

\section{Open Questions}
\subsection*{Question 1}

\subsubsection*{Quoref (Coreference Resolution)}
\begin{itemize}
    \item \textbf{Link:} \url{https://huggingface.co/datasets/allenai/quoref}
    \item \textbf{Explanation:} Quoref evaluates the intrinsic ability to resolve coreferences by requiring models to track references between entities in a passage, essential for coherent understanding of text.
\end{itemize}

\subsubsection*{QA2D (NLI)}
\begin{itemize}
    \item \textbf{Link:} \url{https://huggingface.co/datasets/domenicrosati/QA2D}
    \item \textbf{Explanation:} By transforming QA pairs into premise–hypothesis examples, QA-NLI strips away QA‐specific formats to focus solely on whether the premise entails the hypothesis, directly measuring intrinsic natural language inference ability.
\end{itemize}

\subsubsection*{Stanford\protect/web\_questions (Entity Linking)}
\begin{itemize}
    \item \textbf{Link:} \url{https://huggingface.co/datasets/Stanford/web_questions}
    \item \textbf{Explanation:} WebQuestions consists of 6,642 user‐posed questions whose answers are Freebase entities, so a system must first detect the mention in each question and disambiguate it to the correct Freebase ID before answering—directly evaluating its entity‐linking ability.
\end{itemize}

\pagebreak

\subsection*{Question 2}
\subsection*{(a)}
\subsubsection*{1. Self Consistency}
\paragraph{Description:} 
Rather than greedily decoding a single chain-of-thought (CoT),
we sample $N$ independent reasoning paths and then majority-vote
on the final answers to pick the most frequent one — the intuition being that
the correct answer tends to recur across diverse valid chains

\paragraph{Advantages:}
No extra training, annotations, or auxiliary models required—just sample multiple CoT outputs and vote


\paragraph{Bottlenecks:}
\begin{itemize}
    \item Total forward-pass cost scales linearly with $N$.
    \item Both the GPU’s number-crunching work and its data load/store operations increase in direct proportion to $N$.
\end{itemize}

\paragraph{Parallelization:}
Yes—can batch all $N$ generations in parallel on a single GPU (subject to memory).

\subsubsection*{2. Choosing Chains Based on Verifiers}
\paragraph{Description:}
Similar to self-consistency, but instead of majority-voting on the final answers,
we use a verifier to score each chain and select the one with the highest score.
It can be viewed as a search tree where we only deapth-first search nodes that pass the verifier.
\paragraph{Advantages:}
\begin{itemize}
    \item Can be more efficient than self-consistency, as it avoids generating and evaluating all possible chains.
    \item It also improves correctness by filtering out invalid chains. 
\end{itemize}
\paragraph{Bottlenecks:}
\begin{itemize}
    \item The verifier itself can be a bottleneck, as it needs to be fast and accurate.
    \item Same $N$-fold generation cost as self-consistency.    
\end{itemize}
\paragraph{Parallelization:}
Yes—can batch all $N$ generations in parallel. Since the verification of each chain is independent, we can also parallelize the verification step.

\subsubsection*{3. Increasing Compute Budget}
\paragraph{Description:}
Rather than running a single large model once, you sample $N$ outputs from a smaller model and then use an (automatic) verifier—unit tests, rule‐based checks, or a learned
judge—to select the best candidate. Hassid et al. (2024) showed that under equal computational budgets, this can match or exceed a much larger model’s performance on code-generation tasks 

\paragraph{Advantages:}
\begin{itemize}
    \item We can gain more accuracy with many "weak" models than with a single "strong" model while keeping the same compute budget.
    \item We can plug in any verifier we want, so we can use a simple rule-based verifier or a more complex learned verifier.
\end{itemize}

\paragraph{Bottlenecks:}
\begin{itemize}
    \item Success hinges on verifier quality.
    \item The verifier adds to the overall cost if it is not fast enough.
    \item We need to wait for all $N$ runs to finish before we can start verifying.
\end{itemize}

\paragraph{Parallelization:}
Yes—both sampling and verification can be paralleized. We can run all $N$ generations in parallel on a single GPU (subject to memory). The verification step can also be parallelized, as each verification is independent.


\subsection*{(b)}
I’d go with Self-Consistency. It lets you generate $N$ independent
reasoning chains in one batch on your GPU and then select the
answer that appears most often—no extra models or complex engineering needed.
For a truly challenging scientific problem, you’re unlikely to have
a simple verifier that can check every subtle inference, so building
one can be fragile and time-consuming. Majority-voting across multiple
chains often helps catch random mistakes, but it’s not guaranteed. 
Still, it gives you a reasonable way to improve confidence without
adding significant compute or development overhead.



\pagebreak

\section{Programming Exercise}
Repository URL: \url{https://github.com/dtahory/anlp_ex1}
\subsubsection*{Did the configuration that achieved the best validation accuracy also achieve the best test accuracy?}
Yes. The configurations achieved test accuracy values very close to their corresponding validation accuracy, maintaining the same ranking order.
\begin{itemize}
    \item Configuration 1: Validation Accuracy = 0.8529, Test Accuracy = 0.8214
    \item Configuration 2: Validation Accuracy = 0.8088, Test Accuracy = 0.7826
    \item Configuration 3: Validation Accuracy = 0.7034, Test Accuracy = 0.6898
\end{itemize}

\subsubsection*{Qualitative analysis}
I analyzed 5 validation pairs where the best-performing model (cfg\_1) correctly predicted the label, but the worst-performing model (cfg\_3) did not:

\begin{itemize}
    \item \textbf{Example 1:}
    \begin{itemize}
        \item \textbf{Sentence 1:} Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war.
        \item \textbf{Sentence 2:} His wife said he was "100 percent behind George Bush" and looked forward to using his years of training in the war.
        \item \textbf{Label:} 0, \textbf{cfg\_1:} 0, \textbf{cfg\_3:} 1
    \end{itemize}
    \item \textbf{Example 2:}
    \begin{itemize}
        \item \textbf{Sentence 1:} The dollar was at 116.92 yen against the yen, flat on the session, and at 1.2891 against the Swiss franc, also flat.
        \item \textbf{Sentence 2:} The dollar was at 116.78 yen JPY =, virtually flat on the session, and at 1.2871 against the Swiss franc CHF =, down 0.1 percent.
        \item \textbf{Label:} 0, \textbf{cfg\_1:} 0, \textbf{cfg\_3:} 1
    \end{itemize}
    \item \textbf{Example 3:}
    \begin{itemize}
        \item \textbf{Sentence 1:} No dates have been set for the civil or the criminal trial.
        \item \textbf{Sentence 2:} No dates have been set for the criminal or civil cases, but Shanley has pleaded not guilty.
        \item \textbf{Label:} 0, \textbf{cfg\_1:} 0, \textbf{cfg\_3:} 1
    \end{itemize}
    \item \textbf{Example 4:}
    \begin{itemize}
        \item \textbf{Sentence 1:} "Sanitation is poor ... there could be typhoid and cholera," he said.
        \item \textbf{Sentence 2:} "Sanitation is poor, drinking water is generally left behind ... there could be typhoid and cholera."
        \item \textbf{Label:} 0, \textbf{cfg\_1:} 0, \textbf{cfg\_3:} 1
    \end{itemize}
    \item \textbf{Example 5:}
    \begin{itemize}
        \item \textbf{Sentence 1:} Friday, Stanford (47-15) blanked the Gamecocks 8-0.
        \item \textbf{Sentence 2:} Stanford (46-15) has a team full of such players this season.
        \item \textbf{Label:} 0, \textbf{cfg\_1:} 0, \textbf{cfg\_3:} 1
    \end{itemize}
\end{itemize}

From these examples, I conclude that the lower-performing model (cfg\_3) is overly reliant on superficial token overlap and fails to capture small but crucial semantic cues that distinguish these non-paraphrases. Specifically:

\paragraph{Numeric precision:} It ignores tiny but meaning-changing number shifts (e.g., 116.92 $\rightarrow$ 116.78, 47-15 $\rightarrow$ 46-15).
\paragraph{Clause insertions:} It misses the impact of added qualifiers or after-thoughts (e.g., “but Shanley … pleaded not guilty,” “drinking water … left behind”).
\paragraph{Context shifts:} It treats shared long spans (e.g., “looked forward to using his … training in the war”) as a sign of equivalence, even when the subjects or sentiments differ.

\end{document}